# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password_here
NEO4J_DATABASE=neo4j

# LLM Provider Configuration
# Options: ollama, openai, anthropic, gemini
LLM_PROVIDER=ollama

# Ollama Configuration (Local LLM - Recommended)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b  # Options: llama3.1:8b, llama3.1:70b, mistral, mixtral, etc.
OLLAMA_TIMEOUT=300
OLLAMA_NUM_CTX=8192  # Context window size
OLLAMA_NUM_GPU=1  # Number of GPUs to use (0 for CPU only)
OLLAMA_NUM_THREAD=8  # CPU threads when using CPU

# OpenAI Configuration (Optional - for cloud API)
OPENAI_API_KEY=  # Leave empty if using Ollama
OPENAI_MODEL=gpt-4-turbo

# Anthropic Configuration (Optional - for cloud API)
ANTHROPIC_API_KEY=  # Leave empty if using Ollama
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Gemini Configuration (Optional - for Google Gemini API)
GEMINI_API_KEY=  # Leave empty if using Ollama
GEMINI_MODEL=gemini-2.0-flash-exp  # Options: gemini-2.0-flash-exp, gemini-1.5-pro, etc.

# Embedding Model Configuration
EMBEDDING_PROVIDER=local  # Options: local, openai, ollama
EMBEDDING_MODEL=BAAI/bge-large-en-v1.5  # For local embeddings
OLLAMA_EMBEDDING_MODEL=nomic-embed-text  # For Ollama embeddings
EMBEDDING_DEVICE=cuda  # Options: cuda, cpu, mps
EMBEDDING_BATCH_SIZE=32
EMBEDDING_DIMENSION=1024

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_KEY_SALT=your_random_salt_here
JWT_SECRET_KEY=your_jwt_secret_here
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# Redis (for caching and task queue)
REDIS_URL=redis://localhost:6379/0

# Data Paths
DATA_DIR=./data
PAPERS_DIR=./data/papers
EMBEDDINGS_CACHE_DIR=./data/embeddings
EXPORTS_DIR=./data/exports

# Processing Configuration
MAX_CONCURRENT_EXTRACTIONS=5
EXTRACTION_TIMEOUT=300
DEFAULT_TEMPERATURE=0.0
MAX_TOKENS=4000

# Ollama Model Recommendations by Use Case
# - llama3.1:8b - Fast, good for most tasks, 8GB VRAM
# - llama3.1:70b - Best quality, requires 40GB+ VRAM
# - mistral:7b - Fast alternative, 6GB VRAM
# - mixtral:8x7b - High quality, 26GB VRAM
# - qwen2.5:7b - Good for technical content
# - deepseek-coder:6.7b - Optimized for code/technical papers

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json  # json or text
LOG_FILE=./logs/kg-builder.log

# Feature Flags
ENABLE_ARXIV_INTEGRATION=true
ENABLE_PUBMED_INTEGRATION=false
ENABLE_GRAPHQL=true
ENABLE_WEBSOCKETS=true
