# Data Directory Structure

This directory contains all data files for the KG Builder project.

## ğŸ“ Directory Layout

```
data/
â”œâ”€â”€ papers/              # Research papers (PDFs - local only)
â”‚   â”œâ”€â”€ README.md        # Documentation
â”‚   â”œâ”€â”€ papers_index.json  # Paper metadata (committed to Git)
â”‚   â””â”€â”€ *.pdf            # PDF files (NOT committed - too large)
â”‚
â”œâ”€â”€ exports/             # Extracted knowledge graphs (committed to Git)
â”‚   â”œâ”€â”€ *_knowledge_graph.json    # Individual paper graphs
â”‚   â””â”€â”€ combined_knowledge_graph.json  # Combined graph
â”‚
â”œâ”€â”€ embeddings/          # Cached embeddings (local only)
â”‚   â””â”€â”€ *.npy, *.pkl     # Vector embeddings
â”‚
â””â”€â”€ neo4j/              # Neo4j database (local only)
    â””â”€â”€ data/           # Graph database files
```

## ğŸ”„ What Gets Committed to GitHub

### âœ… Committed (Shared)

1. **Paper Metadata** (`papers_index.json`)
   - Paper titles, authors, arXiv IDs
   - Number of pages, creation dates
   - Extraction statistics
   - **Does NOT include actual PDFs**

2. **Extracted Knowledge Graphs** (`exports/*.json`)
   - Entity and relationship data
   - Metadata and statistics
   - Usually small (<1MB each)
   - The valuable processed output

3. **Directory Structure**
   - README files
   - Directory placeholders

### âŒ NOT Committed (Local Only)

1. **PDF Files** (`papers/*.pdf`)
   - Can be very large (10MB+ each)
   - Copyright considerations
   - Can be re-downloaded from arXiv
   - Listed in `.gitignore`

2. **Neo4j Database** (`neo4j/`)
   - Binary database files
   - Can be regenerated from JSON exports
   - Machine-specific

3. **Embedding Cache** (`embeddings/`)
   - Large binary files
   - Can be regenerated
   - Model-specific

## ğŸ“Š Creating the Papers Index

To create/update the papers index:

```bash
python scripts/create_papers_index.py
```

This generates `papers_index.json` with metadata for all PDFs without including the actual files.

## ğŸ” Example Index Entry

```json
{
  "filename": "2403_11996.pdf",
  "arxiv_id": "2403.11996",
  "title": "Accelerating Scientific Discovery with...",
  "author": "Markus J. Buehler",
  "num_pages": 15,
  "file_size_mb": 12.5,
  "knowledge_graph_extracted": true,
  "num_entities": 42,
  "num_relationships": 58,
  "added_to_index": "2025-11-22T12:00:00"
}
```

## ğŸš€ Reproducing Results

Someone cloning this repo can:

1. **See what papers were used** (via `papers_index.json`)
2. **Download same papers**:
   ```bash
   # Use arXiv ID from index
   python scripts/download_arxiv_paper.py 2403.11996
   ```
3. **Use existing knowledge graphs** (already in `exports/`)
4. **Or re-extract** (if they want):
   ```bash
   python examples/ingest_paper.py data/papers/2403_11996.pdf
   ```

## ğŸ“ Best Practices

### When Adding New Papers

1. Download PDFs to `data/papers/`
2. Extract knowledge graphs
3. Update index:
   ```bash
   python scripts/create_papers_index.py
   ```
4. Commit:
   ```bash
   git add data/papers/papers_index.json
   git add data/exports/*.json
   git commit -m "Add knowledge graphs from 5 new papers"
   ```

### When Sharing Results

```bash
# Commit the processed results, not the PDFs
git add data/papers/papers_index.json
git add data/exports/
git commit -m "Add knowledge graphs for topic X"
git push
```

## ğŸ” Benefits of This Approach

1. **Size Management**: Git repo stays small (<10MB vs potentially GBs)
2. **Copyright Compliance**: Don't redistribute copyrighted PDFs
3. **Reproducibility**: Others can get same papers from arXiv
4. **Sharing Value**: Share the extracted knowledge, not raw papers
5. **Collaboration**: Multiple people can contribute knowledge graphs
6. **Transparency**: Clear record of what papers were processed

## ğŸ“š Storage Estimates

| Item | Size | Committed? |
|------|------|-----------|
| Single PDF | 5-20 MB | âŒ No |
| Knowledge graph JSON | 50-500 KB | âœ… Yes |
| Papers index | 10-50 KB | âœ… Yes |
| 100 PDFs | ~1 GB | âŒ No |
| 100 knowledge graphs | ~10 MB | âœ… Yes |

## ğŸ”„ Syncing Workflow

### On Your Machine

```bash
# Download and process papers
python scripts/search_and_download_papers.py "topic"
python scripts/batch_extract_papers.py

# Create index
python scripts/create_papers_index.py

# Commit results (not PDFs)
git add data/papers/papers_index.json
git add data/exports/
git commit -m "Add knowledge graphs for topic X"
git push
```

### On Another Machine

```bash
# Pull latest results
git pull

# Check what papers exist
cat data/papers/papers_index.json

# Use existing knowledge graphs from data/exports/
# Or download PDFs if needed
python scripts/download_arxiv_paper.py 2403.11996
```

## ğŸ“– Related Documentation

- **Search Guide**: `docs/SEARCH_GUIDE.md` - How to find and download papers
- **Extraction Guide**: `examples/README.md` - How to extract knowledge
- **Main README**: `../README.md` - Project overview

---

**Summary**: PDFs stay local, knowledge graphs go to GitHub. Everyone wins! ğŸ‰
